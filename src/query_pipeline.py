# src/query_pipeline.py
from typing import List, Dict, Any, Optional

# Import the service classes
from src.llm_service import LLMService
from src.embedding_service import EmbeddingService
from src.vector_db import VectorDB

class QueryPipeline:
    """
    Orchestrates the process of handling user queries in the RAG system.
    Routes queries, translates, embeds, retrieves context, and generates answers.
    """
    def __init__(self,
                 llm_service: LLMService,
                 embedding_service: EmbeddingService,
                 vector_db: VectorDB,
                 collection_name: str,
                 top_k: int = 5):
        """
        Initializes the QueryPipeline.

        Args:
            llm_service: An instance of LLMService.
            embedding_service: An instance of EmbeddingService.
            vector_db: An instance of VectorDB.
            collection_name: The name of the Weaviate collection to query.
            top_k: The number of similar items to retrieve from the vector DB.
        """
        self.llm_service = llm_service
        self.embedding_service = embedding_service
        self.vector_db = vector_db
        self.collection_name = collection_name
        self.top_k = top_k

    def process_query(self, query: str, history: List[Dict[str, str]]) -> str:
        """
        Processes a user query through the RAG pipeline.

        Args:
            query: The user's input query string.
            history: The conversation history (list of {'role': 'user'/'assistant', 'content': ...}).

        Returns:
            The final answer generated by the LLM.

        Raises:
            Exception: Propagates exceptions from underlying services if not handled internally.
        """
        print(f"--- Starting Query Pipeline for query: '{query[:50]}...' ---")

        # 1. Route Query
        print("Routing query...")
        try:
            route_decision = self.llm_service.route_query(query)
        except Exception as e:
            print(f"Error during query routing: {e}. Defaulting to UNRELATED.")
            route_decision = "UNRELATED" # Default on error

        context: Optional[str] = None

        # 2. Retrieve Context if Related
        if route_decision == "RELATED":
            print("Route: RELATED. Performing RAG...")
            try:
                # 2a. Translate query (as per original logic)
                print("Translating query to English for search...")
                english_query = self.llm_service.translate_query_to_english(query)
                if english_query == query: # Check if translation actually happened or failed
                    print("Translation may have failed or query was already English.")

                # 2b. Embed translated query
                print("Embedding translated query...")
                query_embedding = self.embedding_service.embed_query(english_query)

                # 2c. Search VectorDB
                print(f"Retrieving top {self.top_k} similar QA pairs from '{self.collection_name}'...")
                # VectorDB search returns list of properties dicts
                similar_objects_props = self.vector_db.search_similar(
                    collection_name=self.collection_name,
                    query_embedding=query_embedding,
                    top_k=self.top_k
                )

                # 2d. Extract Answers for Context
                retrieved_answers = [props["answer"] for props in similar_objects_props if "answer" in props]
                if retrieved_answers:
                    context = "\n---\n".join(retrieved_answers)
                    print(f"Retrieved context snippet: {context[:200]}...")
                else:
                    print("No relevant context found in vector store.")
                    context = None # Ensure context is None if nothing found

            except Exception as e:
                print(f"Error during RAG retrieval steps: {e}")
                # Decide if we should proceed without context or return an error
                print("Proceeding without retrieved context due to error.")
                context = None # Ensure context is None on retrieval error

        elif route_decision == "UNRELATED":
            print("Route: UNRELATED. Skipping RAG.")
            context = None
        else:
             print(f"Warning: Unexpected route decision '{route_decision}'. Defaulting to UNRELATED flow.")
             context = None

        # 3. Generate Final Answer
        print("Generating final answer...")
        try:
            # Pass original query, potentially None context, and history
            answer = self.llm_service.generate_answer(
                query=query,
                context=context,
                history=history
            )
            print(f"Generated answer: {answer[:100]}...")
        except Exception as e:
            print(f"Fatal Error: Failed to generate final answer: {e}")
            # Return a specific error message or re-raise
            answer = "Sorry, I encountered an error while generating the final response."
            # raise # Optionally re-raise

        print("--- Query Pipeline finished ---")
        return answer